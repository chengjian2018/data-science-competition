{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a4850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 230638 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "import os.path as osp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "path = \"d:/data/user_persona\"\n",
    "\n",
    "# 读取数据，简单处理list数据\n",
    "train = pd.read_csv(osp.join(path,'train.txt'), header=None)\n",
    "test = pd.read_csv(osp.join(path,'test.txt'), header=None)\n",
    "\n",
    "train.columns = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "test.columns = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "data = pd.concat([train,test])\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "\n",
    "data['tagid'] = data['tagid'].apply(lambda x:eval(x))\n",
    "data['tagid'] = data['tagid'].apply(lambda x:[str(i) for i in x])\n",
    "\n",
    "# 超参数\n",
    "# embed_size  embedding sizez\n",
    "# MAX_NB_WORDS  tagid中的单词出现次数\n",
    "# MAX_SEQUENCE_LENGTH  输入tagid list的长度\n",
    "embed_size = 64\n",
    "MAX_NB_WORDS = 230637\n",
    "MAX_SEQUENCE_LENGTH = 128 \n",
    "# 训练word2vec，这里可以考虑elmo，bert等预训练\n",
    "w2v_model = Word2Vec(sentences=data['tagid'].tolist(), vector_size=embed_size, window=5, min_count=1,epochs=10)\n",
    "# 这里是划分训练集和测试数据\n",
    "X_train = data[:train.shape[0]]['tagid']\n",
    "X_test = data[train.shape[0]:]['tagid']\n",
    "\n",
    "# 创建词典，利用了tf.keras的API，其实就是编码一下，具体可以看看API的使用方法\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "# 计算一共出现了多少个单词，其实MAX_NB_WORDS我直接就用了这个数据\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "print('Total %s word vectors.' % nb_words)\n",
    "# 构建一个embedding的矩阵，之后输入到模型使用\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "y_categorical = train['label'].values\n",
    "\n",
    "def my_model():\n",
    "    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(nb_words,\n",
    "                         embed_size,\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         weights=[embedding_matrix],\n",
    "                         trainable=False\n",
    "                         )\n",
    "    embed = embedder(embedding_input)\n",
    "    l = LSTM(128)(embed)\n",
    "    flat = BatchNormalization()(l)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(1, activation='sigmoid')(drop)\n",
    "    model = Model(inputs=embedding_input, outputs=main_output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2ee8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1446664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5])\n",
    "a[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b2cbc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n1\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 128, 64)           14760832  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,860,289\n",
      "Trainable params: 99,201\n",
      "Non-trainable params: 14,761,088\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "391/391 [==============================] - 310s 794ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.5883 - val_accuracy: 0.6840\n",
      "Epoch 2/12\n",
      "391/391 [==============================] - 330s 844ms/step - loss: 0.5842 - accuracy: 0.6877 - val_loss: 0.5922 - val_accuracy: 0.6790\n",
      "Epoch 3/12\n",
      "391/391 [==============================] - 324s 829ms/step - loss: 0.5731 - accuracy: 0.6956 - val_loss: 0.5776 - val_accuracy: 0.6909\n",
      "Epoch 4/12\n",
      "391/391 [==============================] - 308s 788ms/step - loss: 0.5664 - accuracy: 0.7012 - val_loss: 0.5694 - val_accuracy: 0.6973\n",
      "Epoch 5/12\n",
      "391/391 [==============================] - 344s 880ms/step - loss: 0.5611 - accuracy: 0.7046 - val_loss: 0.5673 - val_accuracy: 0.6982\n",
      "Epoch 6/12\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 0.5569 - accuracy: 0.7072 - val_loss: 0.5671 - val_accuracy: 0.6995\n",
      "Epoch 7/12\n",
      "391/391 [==============================] - 313s 801ms/step - loss: 0.5528 - accuracy: 0.7114 - val_loss: 0.5671 - val_accuracy: 0.6988\n",
      "Epoch 8/12\n",
      "391/391 [==============================] - 322s 824ms/step - loss: 0.5480 - accuracy: 0.7146 - val_loss: 0.5617 - val_accuracy: 0.7035\n",
      "Epoch 9/12\n",
      "391/391 [==============================] - 311s 796ms/step - loss: 0.5432 - accuracy: 0.7179 - val_loss: 0.5630 - val_accuracy: 0.7016\n",
      "Epoch 10/12\n",
      "391/391 [==============================] - 322s 824ms/step - loss: 0.5376 - accuracy: 0.7225 - val_loss: 0.5694 - val_accuracy: 0.6975\n",
      "Epoch 11/12\n",
      "391/391 [==============================] - 328s 838ms/step - loss: 0.5320 - accuracy: 0.7265 - val_loss: 0.5620 - val_accuracy: 0.7063\n",
      "Epoch 12/12\n",
      "391/391 [==============================] - 346s 885ms/step - loss: 0.5254 - accuracy: 0.7313 - val_loss: 0.5686 - val_accuracy: 0.7049\n",
      "[[0.13725667]\n",
      " [0.12615217]\n",
      " [0.11156613]\n",
      " ...\n",
      " [0.14356847]\n",
      " [0.22081728]\n",
      " [0.10998035]]\n",
      "fold n2\n",
      "Epoch 1/12\n",
      "391/391 [==============================] - 303s 776ms/step - loss: 0.6306 - accuracy: 0.6543 - val_loss: 0.5909 - val_accuracy: 0.6835\n",
      "Epoch 2/12\n",
      "391/391 [==============================] - 298s 763ms/step - loss: 0.5834 - accuracy: 0.6886 - val_loss: 0.5771 - val_accuracy: 0.6909\n",
      "Epoch 3/12\n",
      "391/391 [==============================] - 304s 777ms/step - loss: 0.5720 - accuracy: 0.6962 - val_loss: 0.5753 - val_accuracy: 0.6971\n",
      "Epoch 4/12\n",
      "391/391 [==============================] - 297s 760ms/step - loss: 0.5656 - accuracy: 0.7012 - val_loss: 0.5680 - val_accuracy: 0.6995\n",
      "Epoch 5/12\n",
      "391/391 [==============================] - 300s 768ms/step - loss: 0.5603 - accuracy: 0.7049 - val_loss: 0.5659 - val_accuracy: 0.6999\n",
      "Epoch 6/12\n",
      "391/391 [==============================] - 306s 783ms/step - loss: 0.5556 - accuracy: 0.7086 - val_loss: 0.5665 - val_accuracy: 0.6992\n",
      "Epoch 7/12\n",
      "391/391 [==============================] - 300s 767ms/step - loss: 0.5514 - accuracy: 0.7120 - val_loss: 0.5639 - val_accuracy: 0.7029\n",
      "Epoch 8/12\n",
      "391/391 [==============================] - 302s 772ms/step - loss: 0.5469 - accuracy: 0.7140 - val_loss: 0.5634 - val_accuracy: 0.7027\n",
      "Epoch 9/12\n",
      "391/391 [==============================] - 292s 746ms/step - loss: 0.5423 - accuracy: 0.7197 - val_loss: 0.5641 - val_accuracy: 0.7010\n",
      "Epoch 10/12\n",
      "391/391 [==============================] - 307s 784ms/step - loss: 0.5372 - accuracy: 0.7232 - val_loss: 0.5617 - val_accuracy: 0.7051\n",
      "Epoch 11/12\n",
      "391/391 [==============================] - 358s 917ms/step - loss: 0.5317 - accuracy: 0.7274 - val_loss: 0.5696 - val_accuracy: 0.6999\n",
      "Epoch 12/12\n",
      "391/391 [==============================] - 289s 740ms/step - loss: 0.5250 - accuracy: 0.7320 - val_loss: 0.5673 - val_accuracy: 0.7024\n",
      "[[0.28750019]\n",
      " [0.22601501]\n",
      " [0.20663834]\n",
      " ...\n",
      " [0.30717319]\n",
      " [0.36289097]\n",
      " [0.23688664]]\n",
      "fold n3\n",
      "Epoch 1/12\n",
      "391/391 [==============================] - 271s 694ms/step - loss: 0.6271 - accuracy: 0.6556 - val_loss: 0.5889 - val_accuracy: 0.6854\n",
      "Epoch 2/12\n",
      "391/391 [==============================] - 270s 690ms/step - loss: 0.5826 - accuracy: 0.6885 - val_loss: 0.5799 - val_accuracy: 0.6915\n",
      "Epoch 3/12\n",
      "391/391 [==============================] - 1590s 4s/step - loss: 0.5710 - accuracy: 0.6978 - val_loss: 0.5700 - val_accuracy: 0.6984\n",
      "Epoch 4/12\n",
      "391/391 [==============================] - 384s 983ms/step - loss: 0.5650 - accuracy: 0.7011 - val_loss: 0.5727 - val_accuracy: 0.6939\n",
      "Epoch 5/12\n",
      "391/391 [==============================] - 291s 744ms/step - loss: 0.5594 - accuracy: 0.7057 - val_loss: 0.5657 - val_accuracy: 0.7022\n",
      "Epoch 6/12\n",
      "391/391 [==============================] - 328s 839ms/step - loss: 0.5546 - accuracy: 0.7097 - val_loss: 0.5640 - val_accuracy: 0.7019\n",
      "Epoch 7/12\n",
      "391/391 [==============================] - 1225s 3s/step - loss: 0.5505 - accuracy: 0.7127 - val_loss: 0.5618 - val_accuracy: 0.7049\n",
      "Epoch 8/12\n",
      "391/391 [==============================] - 307s 784ms/step - loss: 0.5460 - accuracy: 0.7163 - val_loss: 0.5608 - val_accuracy: 0.7038\n",
      "Epoch 9/12\n",
      "391/391 [==============================] - 312s 798ms/step - loss: 0.5408 - accuracy: 0.7201 - val_loss: 0.5632 - val_accuracy: 0.7014\n",
      "Epoch 10/12\n",
      "391/391 [==============================] - 328s 840ms/step - loss: 0.5349 - accuracy: 0.7243 - val_loss: 0.5744 - val_accuracy: 0.6996\n",
      "Epoch 11/12\n",
      "391/391 [==============================] - 327s 836ms/step - loss: 0.5290 - accuracy: 0.7289 - val_loss: 0.5605 - val_accuracy: 0.7052\n",
      "Epoch 12/12\n",
      "391/391 [==============================] - 293s 749ms/step - loss: 0.5216 - accuracy: 0.7339 - val_loss: 0.5648 - val_accuracy: 0.7044\n",
      "[[0.40248183]\n",
      " [0.3640325 ]\n",
      " [0.32061452]\n",
      " ...\n",
      " [0.42998453]\n",
      " [0.53461897]\n",
      " [0.34183227]]\n",
      "0.7041133333333334\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 五折交叉验证\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=2019)\n",
    "oof = np.zeros([len(train), 1])\n",
    "predictions = np.zeros([len(test), 1])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "    print(\"fold n{}\".format(fold_ + 1))\n",
    "    model = my_model()\n",
    "    if fold_ == 0:\n",
    "        model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    bst_model_path = \"./models/{}.h5\".format(fold_)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    X_tra, X_val = X_train[trn_idx], X_train[val_idx]\n",
    "    y_tra, y_val = y_categorical[trn_idx], y_categorical[val_idx]\n",
    "\n",
    "    model.fit(X_tra, y_tra,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=12, batch_size=512, shuffle=True,\n",
    "              callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    predictions += model.predict(X_test) / folds.n_splits\n",
    "    print(predictions)\n",
    "    del model\n",
    "\n",
    "train['predict'] = oof\n",
    "train['rank'] = train['predict'].rank()\n",
    "train['p'] = 1\n",
    "train.loc[train['rank'] <= train.shape[0] * 0.5, 'p'] = 0\n",
    "bst_f1_tmp = f1_score(train['label'].values, train['p'].values)\n",
    "print(bst_f1_tmp)\n",
    "\n",
    "submit = test[['pid']]\n",
    "submit['tmp'] = predictions\n",
    "submit.columns = ['user_id', 'tmp']\n",
    "\n",
    "submit['rank'] = submit['tmp'].rank()\n",
    "submit['category_id'] = 1\n",
    "submit.loc[submit['rank'] <= int(submit.shape[0] * 0.5), 'category_id'] = 0\n",
    "\n",
    "print(submit['category_id'].mean())\n",
    "\n",
    "submit[['user_id', 'category_id']].to_csv('submit/lstm_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370c5306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300000.000000\n",
       "mean          0.506396\n",
       "std           0.255147\n",
       "min           0.002079\n",
       "25%           0.303349\n",
       "50%           0.524800\n",
       "75%           0.711148\n",
       "max           0.996728\n",
       "Name: predict, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['predict'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb53313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.2---0.7169251677735838\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.21---0.7186879943294839\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.22---0.7204210843676196\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.23---0.7220048609126476\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.24---0.7236387153281698\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.25---0.7250349922837527\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.26---0.7264263713793699\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.27---0.7275114392678869\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.28---0.7288951047485851\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.29---0.7298693277943028\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.3---0.7307294423006858\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.31---0.7315411417850441\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.32---0.732081791415683\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.33---0.7325756173007592\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.34---0.7329126368224781\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.35---0.7332225637617347\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.36---0.7334590193696147\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.37---0.7336508652274393\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.38---0.7332935119606339\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.39---0.7326565483968238\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.4---0.7322617694146036\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.41---0.731329250182336\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.42---0.7304219184567134\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.43---0.7293245444897838\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.44---0.7276856633747093\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.45---0.7257984747925584\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.46---0.7239716767394914\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.47---0.7218463863038247\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.48---0.7193630468685874\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.49---0.7163793792034762\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.5---0.7134824739801412\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.51---0.7099964006413402\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.52---0.7060694407633183\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.53---0.7017133041911707\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.54---0.6970602331011838\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.55---0.6920610208489049\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.56---0.6863621756807734\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.57---0.6805961477828674\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.58---0.6741451110448384\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.59---0.6670457527775348\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.6---0.6597471220985092\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.61---0.6524075416151913\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.62---0.6444550323035582\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.63---0.6355936140231344\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.64---0.6262990881315543\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.65---0.6165039234174484\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.66---0.6049587448685119\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.67---0.5922006954793839\n",
      "**********************************************************************\n",
      "**********************************************************************\n",
      "0.68---0.579446021172131\n",
      "**********************************************************************\n",
      "**********************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69---0.5668577397439136\n"
     ]
    }
   ],
   "source": [
    "for i in range(20,70):\n",
    "    print(\"*\"*70)\n",
    "    print(\"*\"*70)\n",
    "    bound = i*1.0/100\n",
    "    train['p'] = train['predict'].apply(lambda x:1 if x>=bound else 0)\n",
    "    bst_f1_tmp = f1_score(train['label'].values, train['p'].values)\n",
    "    print(\"{}---{}\".format(bound,bst_f1_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb52adca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean          0.479970\n",
       "std           0.241401\n",
       "min           0.003600\n",
       "25%           0.289518\n",
       "50%           0.484756\n",
       "75%           0.669143\n",
       "max           0.993465\n",
       "Name: tmp, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['tmp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee05489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61362\n"
     ]
    }
   ],
   "source": [
    "bst_f1_tmp = 0.7322617694146036\n",
    "submit = test[['pid']]\n",
    "submit['tmp'] = predictions\n",
    "submit.columns = ['user_id', 'tmp']\n",
    "\n",
    "submit['category_id'] = submit['tmp'].apply(lambda x:1 if x>=0.4 else 0)\n",
    "\n",
    "print(submit['category_id'].mean())\n",
    "\n",
    "submit[['user_id', 'category_id']].to_csv('submit/lstm_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d485d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54721\n",
      "0.53335\n",
      "0.51988\n",
      "0.50628\n",
      "0.49287\n",
      "0.47896\n",
      "0.46549\n",
      "0.45106\n",
      "0.43738\n",
      "0.42385\n",
      "0.41077\n",
      "0.39692\n",
      "0.3833\n",
      "0.369\n",
      "0.35486\n"
     ]
    }
   ],
   "source": [
    "# for bound in range(45,60):\n",
    "#     bound = bound*1.0/100\n",
    "#     submit = test[['pid']]\n",
    "#     submit['tmp'] = predictions\n",
    "#     submit.columns = ['user_id', 'tmp']\n",
    "\n",
    "#     submit['category_id'] = submit['tmp'].apply(lambda x:1 if x>=bound else 0)\n",
    "\n",
    "#     print(submit['category_id'].mean())\n",
    "\n",
    "#     submit[['user_id', 'category_id']].to_csv('submit/lstm_{}.csv'.format(str(bound).split('.')[1]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1634d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digit_house",
   "language": "python",
   "name": "digit_house"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
